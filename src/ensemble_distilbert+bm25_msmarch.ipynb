{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training is done in the `distilbert+bm25_msmarco.ipynb` notebook\n",
    "\n",
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 512\n",
    "model_name = \"distilbert-base-uncased\" \n",
    "dataset = \"msmarco_tiny\"\n",
    "\n",
    "dataset_path = \"../datasets/msmarco_tiny/\"\n",
    "corpus_file = \"tiny_collection.json\"\n",
    "queries_file = \"topics.dl20.txt\"\n",
    "qrels_test_file = \"qrels.dl20-passage.txt\"\n",
    "training_set = \"msmarco_triples.train.tiny.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ab/.pyenv/versions/3.11.6/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sentence_transformers import losses, models, SentenceTransformer\n",
    "from beir import util, LoggingHandler\n",
    "from beir.datasets.data_loader import GenericDataLoader\n",
    "from beir.retrieval.search.lexical import BM25Search as BM25\n",
    "from beir.retrieval.evaluation import EvaluateRetrieval\n",
    "from beir.retrieval.train import TrainRetriever\n",
    "import pathlib, os, tqdm\n",
    "import logging\n",
    "\n",
    "#### Just some code to print debug information to stdout\n",
    "logging.basicConfig(format='%(asctime)s - %(message)s',\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "                    level=logging.INFO,\n",
    "                    handlers=[LoggingHandler()])\n",
    "#### /print debug information to stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-02 13:18:34 - Loading Corpus...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5183/5183 [00:00<00:00, 16771.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-02 13:18:35 - Loaded 5183 TRAIN Documents.\n",
      "2024-06-02 13:18:35 - Doc Example: {'text': 'Alterations of the architecture of cerebral white matter in the developing human brain can affect cortical development and result in functional disabilities. A line scan diffusion-weighted magnetic resonance imaging (MRI) sequence with diffusion tensor analysis was applied to measure the apparent diffusion coefficient, to calculate relative anisotropy, and to delineate three-dimensional fiber architecture in cerebral white matter in preterm (n = 17) and full-term infants (n = 7). To assess effects of prematurity on cerebral white matter development, early gestation preterm infants (n = 10) were studied a second time at term. In the central white matter the mean apparent diffusion coefficient at 28 wk was high, 1.8 microm2/ms, and decreased toward term to 1.2 microm2/ms. In the posterior limb of the internal capsule, the mean apparent diffusion coefficients at both times were similar (1.2 versus 1.1 microm2/ms). Relative anisotropy was higher the closer birth was to term with greater absolute values in the internal capsule than in the central white matter. Preterm infants at term showed higher mean diffusion coefficients in the central white matter (1.4 +/- 0.24 versus 1.15 +/- 0.09 microm2/ms, p = 0.016) and lower relative anisotropy in both areas compared with full-term infants (white matter, 10.9 +/- 0.6 versus 22.9 +/- 3.0%, p = 0.001; internal capsule, 24.0 +/- 4.44 versus 33.1 +/- 0.6% p = 0.006). Nonmyelinated fibers in the corpus callosum were visible by diffusion tensor MRI as early as 28 wk; full-term and preterm infants at term showed marked differences in white matter fiber organization. The data indicate that quantitative assessment of water diffusion by diffusion tensor MRI provides insight into microstructural development in cerebral white matter in living infants.', 'title': 'Microstructural development of human newborn cerebral white matter assessed in vivo by diffusion tensor magnetic resonance imaging.'}\n",
      "2024-06-02 13:18:35 - Loading Queries...\n",
      "2024-06-02 13:18:35 - Loaded 809 TRAIN Queries.\n",
      "2024-06-02 13:18:35 - Query Example: 0-dimensional biomaterials lack inductive properties.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data_path = f\"../datasets/{dataset}\"\n",
    "corpus, queries, qrels = GenericDataLoader(data_path).load(split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"name\" : \"ab\",\n",
      "  \"cluster_name\" : \"elasticsearch\",\n",
      "  \"cluster_uuid\" : \"JS2mz8c3RdOol8iLxCkFHA\",\n",
      "  \"version\" : {\n",
      "    \"number\" : \"7.9.2\",\n",
      "    \"build_flavor\" : \"oss\",\n",
      "    \"build_type\" : \"tar\",\n",
      "    \"build_hash\" : \"d34da0ea4a966c4e49417f2da2f244e3e97b4e6e\",\n",
      "    \"build_date\" : \"2020-09-23T00:45:33.626720Z\",\n",
      "    \"build_snapshot\" : false,\n",
      "    \"lucene_version\" : \"8.6.2\",\n",
      "    \"minimum_wire_compatibility_version\" : \"6.8.0\",\n",
      "    \"minimum_index_compatibility_version\" : \"6.0.0-beta1\"\n",
      "  },\n",
      "  \"tagline\" : \"You Know, for Search\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "curl -sX GET \"localhost:9200/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-02 13:19:57 - Activating Elasticsearch....\n",
      "2024-06-02 13:19:57 - Elastic Search Credentials: {'hostname': 'localhost', 'index_name': 'scifact', 'keys': {'title': 'title', 'body': 'txt'}, 'timeout': 100, 'retry_on_timeout': True, 'maxsize': 24, 'number_of_shards': 1, 'language': 'english'}\n",
      "2024-06-02 13:19:57 - Deleting previous Elasticsearch-Index named - scifact\n",
      "2024-06-02 13:19:59 - Creating fresh Elasticsearch-Index named - scifact\n"
     ]
    }
   ],
   "source": [
    "#### Lexical Retrieval using Bm25 (Elasticsearch) ####\n",
    "\n",
    "## elasticsearch settings\n",
    "hostname = \"localhost\" #localhost\n",
    "index_name = dataset # scifact\n",
    "initialize = True # True - Delete existing index and re-index all documents from scratch \n",
    "\n",
    "number_of_shards = 1\n",
    "model = BM25(index_name=index_name, hostname=hostname, initialize=initialize, number_of_shards=number_of_shards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5183 [00:00<?, ?docs/s]             \n",
      "Retrieve Hard Negatives using BM25: 100%|██████████| 809/809 [00:08<00:00, 93.34it/s] \n"
     ]
    }
   ],
   "source": [
    "bm25 = EvaluateRetrieval(model)\n",
    "\n",
    "#### Index passages into the index (seperately)\n",
    "bm25.retriever.index(corpus)\n",
    "\n",
    "triplets = []\n",
    "qids = list(qrels) \n",
    "hard_negatives_max = 10\n",
    "\n",
    "#### Retrieve BM25 hard negatives => Given a positive document, find most similar lexical documents\n",
    "for idx in tqdm.tqdm(range(len(qids)), desc=\"Retrieve Hard Negatives using BM25\"):\n",
    "    query_id, query_text = qids[idx], queries[qids[idx]]\n",
    "    pos_docs = [doc_id for doc_id in qrels[query_id] if qrels[query_id][doc_id] > 0]\n",
    "    pos_doc_texts = [corpus[doc_id][\"title\"] + \" \" + corpus[doc_id][\"text\"] for doc_id in pos_docs]\n",
    "    hits = bm25.retriever.es.lexical_multisearch(texts=pos_doc_texts, top_hits=hard_negatives_max+1)\n",
    "    for (pos_text, hit) in zip(pos_doc_texts, hits):\n",
    "        for (neg_id, _) in hit.get(\"hits\"):\n",
    "            if neg_id not in pos_docs:\n",
    "                neg_text = corpus[neg_id][\"title\"] + \" \" + corpus[neg_id][\"text\"]\n",
    "                triplets.append([query_text, pos_text, neg_text])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ab/.pyenv/versions/3.11.6/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-31 01:32:47 - Use pytorch device_name: cuda\n"
     ]
    }
   ],
   "source": [
    "#### Provide any sentence-transformers or HF model\n",
    "word_embedding_model = models.Transformer(model_name, max_seq_length=max_seq_length)\n",
    "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n",
    "model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n",
    "\n",
    "#### Provide a high batch-size to train better with triplets!\n",
    "retriever = TrainRetriever(model=model, batch_size=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Provide model save path\n",
    "model_save_path = os.path.join(os.getcwd(), \"../output\", \"{}-v2-{}-bm25-hard-negs\".format(model_name, dataset))\n",
    "os.makedirs(model_save_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9121, list)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(triplets), type(triplets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding Input Examples: 100%|██████████| 761/761 [00:00<00:00, 94990.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-31 01:37:15 - Loaded 9121 training pairs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#### Validation set can't be provider to retriever.fit()\n",
    "\n",
    "# train_cutoff = 2800\n",
    "# #### Prepare triplets-train samples\n",
    "# valid_samples = retriever.load_train_triplets(triplets=triplets[:train_cutoff])\n",
    "# valid_dataloader = retriever.prepare_train_triplets(valid_samples)\n",
    "\n",
    "#### Prepare triplets-validation samples\n",
    "train_samples = retriever.load_train_triplets(triplets=triplets)\n",
    "train_dataloader = retriever.prepare_train_triplets(train_samples)\n",
    "\n",
    "#### Training SBERT with cosine-product\n",
    "train_loss = losses.MultipleNegativesRankingLoss(model=retriever.model)\n",
    "\n",
    "#### Prepare dev evaluator\n",
    "# ir_evaluator = retriever.load_ir_evaluator(dev_corpus, dev_queries, dev_qrels)\n",
    "\n",
    "#### If no dev set is present from above use dummy evaluator\n",
    "ir_evaluator = retriever.load_dummy_evaluator()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-31 01:37:16 - Starting to Train...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ab/.pyenv/versions/3.11.6/lib/python3.11/site-packages/transformers/optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='40' max='7600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  40/7600 00:15 < 52:12, 2.41 it/s, Epoch 0.05/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m evaluation_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5000\u001b[39m\n\u001b[1;32m      4\u001b[0m warmup_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(train_samples) \u001b[38;5;241m*\u001b[39m num_epochs \u001b[38;5;241m/\u001b[39m retriever\u001b[38;5;241m.\u001b[39mbatch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.1\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m \u001b[43mretriever\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_objectives\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loss\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                \u001b[49m\u001b[43mevaluator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mir_evaluator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m                \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m                \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_save_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m                \u001b[49m\u001b[43mwarmup_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwarmup_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m                \u001b[49m\u001b[43mevaluation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevaluation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m                \u001b[49m\u001b[43muse_amp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.6/lib/python3.11/site-packages/beir/retrieval/train.py:134\u001b[0m, in \u001b[0;36mTrainRetriever.fit\u001b[0;34m(self, train_objectives, evaluator, epochs, steps_per_epoch, scheduler, warmup_steps, optimizer_class, optimizer_params, weight_decay, evaluation_steps, output_path, save_best_model, max_grad_norm, use_amp, callback, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, \n\u001b[1;32m    114\u001b[0m         train_objectives: Iterable[Tuple[DataLoader, nn\u001b[38;5;241m.\u001b[39mModule]],\n\u001b[1;32m    115\u001b[0m         evaluator: SentenceEvaluator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    130\u001b[0m     \n\u001b[1;32m    131\u001b[0m     \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m    132\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting to Train...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 134\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_objectives\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_objectives\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m            \u001b[49m\u001b[43mevaluator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevaluator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[43m            \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[43m            \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwarmup_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwarmup_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m            \u001b[49m\u001b[43moptimizer_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m            \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m            \u001b[49m\u001b[43moptimizer_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m            \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m            \u001b[49m\u001b[43mevaluation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevaluation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m            \u001b[49m\u001b[43msave_best_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_best_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmax_grad_norm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_grad_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m            \u001b[49m\u001b[43muse_amp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_amp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.6/lib/python3.11/site-packages/sentence_transformers/fit_mixin.py:371\u001b[0m, in \u001b[0;36mFitMixin.fit\u001b[0;34m(self, train_objectives, evaluator, epochs, steps_per_epoch, scheduler, warmup_steps, optimizer_class, optimizer_params, weight_decay, evaluation_steps, output_path, save_best_model, max_grad_norm, use_amp, callback, show_progress_bar, checkpoint_path, checkpoint_save_steps, checkpoint_save_total_limit)\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    369\u001b[0m     trainer\u001b[38;5;241m.\u001b[39madd_callback(SaveModelCallback(output_path, evaluator, save_best_model))\n\u001b[0;32m--> 371\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.6/lib/python3.11/site-packages/transformers/trainer.py:1885\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1883\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1884\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1885\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.6/lib/python3.11/site-packages/transformers/trainer.py:2221\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2215\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[1;32m   2216\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   2218\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2219\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2220\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m-> 2221\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misinf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss_step\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   2222\u001b[0m ):\n\u001b[1;32m   2223\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2224\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[1;32m   2225\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# #### Configure Train params\n",
    "# num_epochs = 10\n",
    "# evaluation_steps = 5000\n",
    "# warmup_steps = int(len(train_samples) * num_epochs / retriever.batch_size * 0.1)\n",
    "\n",
    "# retriever.fit(train_objectives=[(train_dataloader, train_loss)], \n",
    "#                 evaluator=ir_evaluator, \n",
    "#                 epochs=num_epochs,\n",
    "#                 output_path=model_save_path,\n",
    "#                 warmup_steps=warmup_steps,\n",
    "#                 evaluation_steps=evaluation_steps,\n",
    "#                 use_amp=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate-distilBert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-02 13:20:32 - Loading Corpus...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5183/5183 [00:00<00:00, 11253.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-02 13:20:33 - Loaded 5183 TEST Documents.\n",
      "2024-06-02 13:20:33 - Doc Example: {'text': 'Alterations of the architecture of cerebral white matter in the developing human brain can affect cortical development and result in functional disabilities. A line scan diffusion-weighted magnetic resonance imaging (MRI) sequence with diffusion tensor analysis was applied to measure the apparent diffusion coefficient, to calculate relative anisotropy, and to delineate three-dimensional fiber architecture in cerebral white matter in preterm (n = 17) and full-term infants (n = 7). To assess effects of prematurity on cerebral white matter development, early gestation preterm infants (n = 10) were studied a second time at term. In the central white matter the mean apparent diffusion coefficient at 28 wk was high, 1.8 microm2/ms, and decreased toward term to 1.2 microm2/ms. In the posterior limb of the internal capsule, the mean apparent diffusion coefficients at both times were similar (1.2 versus 1.1 microm2/ms). Relative anisotropy was higher the closer birth was to term with greater absolute values in the internal capsule than in the central white matter. Preterm infants at term showed higher mean diffusion coefficients in the central white matter (1.4 +/- 0.24 versus 1.15 +/- 0.09 microm2/ms, p = 0.016) and lower relative anisotropy in both areas compared with full-term infants (white matter, 10.9 +/- 0.6 versus 22.9 +/- 3.0%, p = 0.001; internal capsule, 24.0 +/- 4.44 versus 33.1 +/- 0.6% p = 0.006). Nonmyelinated fibers in the corpus callosum were visible by diffusion tensor MRI as early as 28 wk; full-term and preterm infants at term showed marked differences in white matter fiber organization. The data indicate that quantitative assessment of water diffusion by diffusion tensor MRI provides insight into microstructural development in cerebral white matter in living infants.', 'title': 'Microstructural development of human newborn cerebral white matter assessed in vivo by diffusion tensor magnetic resonance imaging.'}\n",
      "2024-06-02 13:20:33 - Loading Queries...\n",
      "2024-06-02 13:20:33 - Loaded 300 TEST Queries.\n",
      "2024-06-02 13:20:33 - Query Example: 0-dimensional biomaterials show inductive properties.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Loading test set\n",
    "corpus, queries, qrels = GenericDataLoader(data_path).load(split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-02 13:20:54 - Use pytorch device_name: cuda\n",
      "2024-06-02 13:20:54 - Load pretrained SentenceTransformer: /mnt/c/D_drive/UCSD/Quarters/Q3/DSC253-Adv_txt_mining/Project/slm4search/src/../output/distilbert-base-uncased-v2-scifact-bm25-hard-negs\n",
      "2024-06-02 13:21:11 - Encoding Queries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 3/3 [00:17<00:00,  5.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-02 13:21:28 - Sorting Corpus by document length (Longest first)...\n",
      "2024-06-02 13:21:28 - Encoding Corpus in batches... Warning: This might take a while!\n",
      "2024-06-02 13:21:28 - Scoring Function: Cosine Similarity (cos_sim)\n",
      "2024-06-02 13:21:28 - Encoding Batch 1/1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches:  32%|███▏      | 13/41 [00:12<00:26,  1.07it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m retriever \u001b[38;5;241m=\u001b[39m EvaluateRetrieval(model, score_function\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcos_sim\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m#### Retrieve dense results (format of results is identical to qrels)\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mretriever\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqueries\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.6/lib/python3.11/site-packages/beir/retrieval/evaluation.py:20\u001b[0m, in \u001b[0;36mEvaluateRetrieval.retrieve\u001b[0;34m(self, corpus, queries, **kwargs)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretriever:\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel/Technique has not been provided!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretriever\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqueries\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscore_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.6/lib/python3.11/site-packages/beir/retrieval/search/dense/exact_search.py:61\u001b[0m, in \u001b[0;36mDenseRetrievalExactSearch.search\u001b[0;34m(self, corpus, queries, top_k, score_function, return_sorted, **kwargs)\u001b[0m\n\u001b[1;32m     58\u001b[0m corpus_end_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(corpus_start_idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcorpus_chunk_size, \u001b[38;5;28mlen\u001b[39m(corpus))\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# Encode chunk of corpus    \u001b[39;00m\n\u001b[0;32m---> 61\u001b[0m sub_corpus_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_corpus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcorpus\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcorpus_start_idx\u001b[49m\u001b[43m:\u001b[49m\u001b[43mcorpus_end_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconvert_to_tensor\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensor\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# Compute similarites using either cosine-similarity or dot product\u001b[39;00m\n\u001b[1;32m     69\u001b[0m cos_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscore_functions[score_function](query_embeddings, sub_corpus_embeddings)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.6/lib/python3.11/site-packages/beir/retrieval/models/sentence_bert.py:53\u001b[0m, in \u001b[0;36mSentenceBERT.encode_corpus\u001b[0;34m(self, corpus, batch_size, **kwargs)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [(doc[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msep \u001b[38;5;241m+\u001b[39m doc[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m doc \u001b[38;5;28;01melse\u001b[39;00m doc[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m corpus]\n\u001b[0;32m---> 53\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdoc_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.6/lib/python3.11/site-packages/sentence_transformers/SentenceTransformer.py:512\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[0;34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings)\u001b[0m\n\u001b[1;32m    503\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m features:\n\u001b[1;32m    504\u001b[0m             features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(\n\u001b[1;32m    505\u001b[0m                 (\n\u001b[1;32m    506\u001b[0m                     features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    509\u001b[0m                 \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    510\u001b[0m             )\n\u001b[0;32m--> 512\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[43mbatch_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    513\u001b[0m features\u001b[38;5;241m.\u001b[39mupdate(extra_features)\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.6/lib/python3.11/site-packages/sentence_transformers/util.py:561\u001b[0m, in \u001b[0;36mbatch_to_device\u001b[0;34m(batch, target_device)\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m batch:\n\u001b[1;32m    560\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(batch[key], Tensor):\n\u001b[0;32m--> 561\u001b[0m         batch[key] \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget_device\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m batch\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from beir.retrieval.evaluation import EvaluateRetrieval\n",
    "from beir.retrieval import models\n",
    "from beir.retrieval.search.dense import DenseRetrievalExactSearch as DRES\n",
    "\n",
    "## Load retriever from saved model\n",
    "\n",
    "model = DRES(models.SentenceBERT(model_save_path), batch_size=128)\n",
    "retriever = EvaluateRetrieval(model, score_function=\"cos_sim\")\n",
    "\n",
    "#### Retrieve dense results (format of results is identical to qrels)\n",
    "results = retriever.retrieve(corpus, queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "output_path = \"../output/\"\n",
    "with open(f\"{output_path}{dataset}_distilBert_results.json\", 'w') as f:\n",
    "    json.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "output_path = \"../output/\"\n",
    "with open(f\"{output_path}{dataset}_distilBert_results.json\", 'r') as f:\n",
    "    results = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-02 13:21:46 - Retriever evaluation for k in: [1, 3, 5, 10, 100, 1000]\n",
      "2024-06-02 13:21:46 - For evaluation, we ignore identical query and document ids (default), please explicitly set ``ignore_identical_ids=False`` to ignore this.\n",
      "2024-06-02 13:21:46 - \n",
      "\n",
      "2024-06-02 13:21:46 - NDCG@1: 0.6767\n",
      "2024-06-02 13:21:46 - NDCG@3: 0.7256\n",
      "2024-06-02 13:21:46 - NDCG@5: 0.7348\n",
      "2024-06-02 13:21:46 - NDCG@10: 0.7430\n",
      "2024-06-02 13:21:46 - NDCG@100: 0.7634\n",
      "2024-06-02 13:21:46 - NDCG@1000: 0.7725\n",
      "2024-06-02 13:21:46 - \n",
      "\n",
      "2024-06-02 13:21:46 - MAP@1: 0.6409\n",
      "2024-06-02 13:21:46 - MAP@3: 0.7052\n",
      "2024-06-02 13:21:46 - MAP@5: 0.7131\n",
      "2024-06-02 13:21:46 - MAP@10: 0.7168\n",
      "2024-06-02 13:21:46 - MAP@100: 0.7207\n",
      "2024-06-02 13:21:46 - MAP@1000: 0.7210\n",
      "2024-06-02 13:21:46 - \n",
      "\n",
      "2024-06-02 13:21:46 - Recall@1: 0.6409\n",
      "2024-06-02 13:21:46 - Recall@3: 0.7594\n",
      "2024-06-02 13:21:46 - Recall@5: 0.7855\n",
      "2024-06-02 13:21:46 - Recall@10: 0.8089\n",
      "2024-06-02 13:21:46 - Recall@100: 0.9061\n",
      "2024-06-02 13:21:46 - Recall@1000: 0.9782\n",
      "2024-06-02 13:21:46 - \n",
      "\n",
      "2024-06-02 13:21:46 - P@1: 0.6767\n",
      "2024-06-02 13:21:46 - P@3: 0.2767\n",
      "2024-06-02 13:21:46 - P@5: 0.1727\n",
      "2024-06-02 13:21:46 - P@10: 0.0893\n",
      "2024-06-02 13:21:46 - P@100: 0.0101\n",
      "2024-06-02 13:21:46 - P@1000: 0.0011\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'NDCG@1': 0.67667,\n",
       "  'NDCG@3': 0.72557,\n",
       "  'NDCG@5': 0.73475,\n",
       "  'NDCG@10': 0.74305,\n",
       "  'NDCG@100': 0.76343,\n",
       "  'NDCG@1000': 0.77253},\n",
       " {'MAP@1': 0.64094,\n",
       "  'MAP@3': 0.70522,\n",
       "  'MAP@5': 0.71306,\n",
       "  'MAP@10': 0.71684,\n",
       "  'MAP@100': 0.72071,\n",
       "  'MAP@1000': 0.72102},\n",
       " {'Recall@1': 0.64094,\n",
       "  'Recall@3': 0.75939,\n",
       "  'Recall@5': 0.7855,\n",
       "  'Recall@10': 0.80894,\n",
       "  'Recall@100': 0.90611,\n",
       "  'Recall@1000': 0.97822},\n",
       " {'P@1': 0.67667,\n",
       "  'P@3': 0.27667,\n",
       "  'P@5': 0.17267,\n",
       "  'P@10': 0.08933,\n",
       "  'P@100': 0.01013,\n",
       "  'P@1000': 0.0011})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### Evaluate your retrieval using NDCG@k, MAP@K ...\n",
    "logging.info(\"Retriever evaluation for k in: {}\".format(retriever.k_values))\n",
    "ndcg, _map, recall, precision = retriever.evaluate(qrels, results, retriever.k_values)\n",
    "ndcg, _map, recall, precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate-bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-02 13:21:48 - Activating Elasticsearch....\n",
      "2024-06-02 13:21:48 - Elastic Search Credentials: {'hostname': 'localhost', 'index_name': 'scifact', 'keys': {'title': 'title', 'body': 'txt'}, 'timeout': 100, 'retry_on_timeout': True, 'maxsize': 24, 'number_of_shards': 'default', 'language': 'english'}\n",
      "2024-06-02 13:21:48 - Deleting previous Elasticsearch-Index named - scifact\n",
      "2024-06-02 13:21:51 - Creating fresh Elasticsearch-Index named - scifact\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5183 [00:00<?, ?docs/s]             \n",
      "que: 100%|██████████| 3/3 [00:03<00:00,  1.09s/it]\n"
     ]
    }
   ],
   "source": [
    "from beir.retrieval.search.lexical import BM25Search as BM25\n",
    "from beir.retrieval.evaluation import EvaluateRetrieval\n",
    "\n",
    "model_bm25 = BM25(index_name=index_name, hostname=hostname, initialize=initialize)\n",
    "retriever_bm25 = EvaluateRetrieval(model_bm25)\n",
    "\n",
    "#### Retrieve dense results (format of results is identical to qrels)\n",
    "results_bm25 = retriever_bm25.retrieve(corpus, queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-02 13:22:04 - For evaluation, we ignore identical query and document ids (default), please explicitly set ``ignore_identical_ids=False`` to ignore this.\n",
      "2024-06-02 13:22:04 - \n",
      "\n",
      "2024-06-02 13:22:04 - NDCG@1: 0.5767\n",
      "2024-06-02 13:22:04 - NDCG@3: 0.6345\n",
      "2024-06-02 13:22:04 - NDCG@5: 0.6632\n",
      "2024-06-02 13:22:04 - NDCG@10: 0.6886\n",
      "2024-06-02 13:22:04 - NDCG@100: 0.7119\n",
      "2024-06-02 13:22:04 - NDCG@1000: 0.7192\n",
      "2024-06-02 13:22:04 - \n",
      "\n",
      "2024-06-02 13:22:04 - MAP@1: 0.5559\n",
      "2024-06-02 13:22:04 - MAP@3: 0.6127\n",
      "2024-06-02 13:22:04 - MAP@5: 0.6297\n",
      "2024-06-02 13:22:04 - MAP@10: 0.6422\n",
      "2024-06-02 13:22:04 - MAP@100: 0.6476\n",
      "2024-06-02 13:22:04 - MAP@1000: 0.6479\n",
      "2024-06-02 13:22:04 - \n",
      "\n",
      "2024-06-02 13:22:04 - Recall@1: 0.5559\n",
      "2024-06-02 13:22:04 - Recall@3: 0.6759\n",
      "2024-06-02 13:22:04 - Recall@5: 0.7446\n",
      "2024-06-02 13:22:04 - Recall@10: 0.8164\n",
      "2024-06-02 13:22:04 - Recall@100: 0.9192\n",
      "2024-06-02 13:22:04 - Recall@1000: 0.9767\n",
      "2024-06-02 13:22:04 - \n",
      "\n",
      "2024-06-02 13:22:04 - P@1: 0.5767\n",
      "2024-06-02 13:22:04 - P@3: 0.2400\n",
      "2024-06-02 13:22:04 - P@5: 0.1613\n",
      "2024-06-02 13:22:04 - P@10: 0.0903\n",
      "2024-06-02 13:22:04 - P@100: 0.0104\n",
      "2024-06-02 13:22:04 - P@1000: 0.0011\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'NDCG@1': 0.57667,\n",
       "  'NDCG@3': 0.63447,\n",
       "  'NDCG@5': 0.66322,\n",
       "  'NDCG@10': 0.68862,\n",
       "  'NDCG@100': 0.71187,\n",
       "  'NDCG@1000': 0.71921},\n",
       " {'MAP@1': 0.55594,\n",
       "  'MAP@3': 0.61266,\n",
       "  'MAP@5': 0.62966,\n",
       "  'MAP@10': 0.64216,\n",
       "  'MAP@100': 0.64763,\n",
       "  'MAP@1000': 0.64792},\n",
       " {'Recall@1': 0.55594,\n",
       "  'Recall@3': 0.67594,\n",
       "  'Recall@5': 0.74456,\n",
       "  'Recall@10': 0.81644,\n",
       "  'Recall@100': 0.91922,\n",
       "  'Recall@1000': 0.97667},\n",
       " {'P@1': 0.57667,\n",
       "  'P@3': 0.24,\n",
       "  'P@5': 0.16133,\n",
       "  'P@10': 0.09033,\n",
       "  'P@100': 0.0104,\n",
       "  'P@1000': 0.00111})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### Evaluate your retrieval using NDCG@k, MAP@K ...\n",
    "ndcg, _map, recall, precision = retriever_bm25.evaluate(qrels, results_bm25, retriever_bm25.k_values)\n",
    "ndcg, _map, recall, precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.00429678987711668, 0.9599123001098633, 0.5297587, 120.60852)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_maxmin(results):\n",
    "    max_score = -1\n",
    "    min_score = 999999\n",
    "    for q_id, q in results.items():\n",
    "        for doc_id, score in q.items():\n",
    "            max_score = max(score, max_score)\n",
    "            min_score = min(score, min_score)\n",
    "\n",
    "    return min_score, max_score\n",
    "\n",
    "# Get range to normalize both\n",
    "min_distilbert_score, max_distilbert_score = get_maxmin(results)\n",
    "min_bm25_score, max_bm25_score = get_maxmin(results_bm25)\n",
    "\n",
    "min_distilbert_score, max_distilbert_score, min_bm25_score, max_bm25_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize\n",
    "def normalize_results(results, min_score, max_score):\n",
    "    for q_id, q in results.items():\n",
    "        for doc_id, score in q.items():\n",
    "            results[q_id][doc_id] = (score-min_score)/(max_score-min_score)\n",
    "\n",
    "    return results\n",
    "\n",
    "results = normalize_results(results, min_distilbert_score, max_distilbert_score)\n",
    "results_bm25 = normalize_results(results_bm25, min_bm25_score, max_bm25_score)\n",
    "# results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_score(x,y):\n",
    "    mu = 0.5\n",
    "    return mu*x + (1-mu)*y\n",
    "\n",
    "combined_result = {}\n",
    "\n",
    "for q_id_1, q_1 in results.items():\n",
    "        combined_result[q_id_1] = {}\n",
    "        for doc_id_1, score_1 in q_1.items():\n",
    "            \n",
    "            score_2 = 0\n",
    "            if results_bm25[q_id_1].get(doc_id_1,None)!=None:\n",
    "                score_2 = results_bm25[q_id_1][doc_id_1]\n",
    "                del results_bm25[q_id_1][doc_id_1] # So that same query-doc pair is not added to combined result twice\n",
    "            \n",
    "            combined_score = ensemble_score(score_1, score_2)\n",
    "            combined_result[q_id_1][doc_id_1] = combined_score\n",
    "\n",
    "\n",
    "# Now add remaining bm25 results in combined dict\n",
    "for q_id_2, q_2 in results_bm25.items():\n",
    "    for doc_id_2, score_2 in q_2.items():\n",
    "         score_1 = 0\n",
    "         combined_score = ensemble_score(score_1, score_2)\n",
    "         combined_result[q_id_1][doc_id_1] = combined_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-02 13:22:31 - For evaluation, we ignore identical query and document ids (default), please explicitly set ``ignore_identical_ids=False`` to ignore this.\n",
      "2024-06-02 13:22:31 - \n",
      "\n",
      "2024-06-02 13:22:31 - NDCG@1: 0.7400\n",
      "2024-06-02 13:22:31 - NDCG@3: 0.7794\n",
      "2024-06-02 13:22:31 - NDCG@5: 0.7998\n",
      "2024-06-02 13:22:31 - NDCG@10: 0.8109\n",
      "2024-06-02 13:22:31 - NDCG@100: 0.8260\n",
      "2024-06-02 13:22:31 - NDCG@1000: 0.8296\n",
      "2024-06-02 13:22:31 - \n",
      "\n",
      "2024-06-02 13:22:31 - MAP@1: 0.7026\n",
      "2024-06-02 13:22:31 - MAP@3: 0.7611\n",
      "2024-06-02 13:22:31 - MAP@5: 0.7761\n",
      "2024-06-02 13:22:31 - MAP@10: 0.7814\n",
      "2024-06-02 13:22:31 - MAP@100: 0.7852\n",
      "2024-06-02 13:22:31 - MAP@1000: 0.7854\n",
      "2024-06-02 13:22:31 - \n",
      "\n",
      "2024-06-02 13:22:31 - Recall@1: 0.7026\n",
      "2024-06-02 13:22:31 - Recall@3: 0.8061\n",
      "2024-06-02 13:22:31 - Recall@5: 0.8573\n",
      "2024-06-02 13:22:31 - Recall@10: 0.8885\n",
      "2024-06-02 13:22:31 - Recall@100: 0.9519\n",
      "2024-06-02 13:22:31 - Recall@1000: 0.9782\n",
      "2024-06-02 13:22:31 - \n",
      "\n",
      "2024-06-02 13:22:31 - P@1: 0.7400\n",
      "2024-06-02 13:22:31 - P@3: 0.2922\n",
      "2024-06-02 13:22:31 - P@5: 0.1887\n",
      "2024-06-02 13:22:31 - P@10: 0.0987\n",
      "2024-06-02 13:22:31 - P@100: 0.0107\n",
      "2024-06-02 13:22:31 - P@1000: 0.0011\n"
     ]
    }
   ],
   "source": [
    "ndcg, _map, recall, precision = retriever_bm25.evaluate(qrels, combined_result, retriever_bm25.k_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
